name: robots
type: Page
description: Robots.txt for search engine crawlers

# The component content (must be at root level, not nested in config)
component: |
User-agent: *
  {{#if disallowAll}}
Disallow: /
  {{else}}
Allow: /
  {{#if disallowPaths}}
  {{#each disallowPaths}}
Disallow: {{this}}
  {{/each}}
  {{/if}}
  {{/if}}

  {{#if crawlDelay}}
Crawl-delay: {{crawlDelay}}
  {{/if}}

  {{#if sitemap}}
Sitemap: {{sitemap}}
  {{/if}}

# Page configuration (all properties at root level, no config wrapper)
route:
  path: /robots.txt
  methods: [GET]
  auth: none

renderMode: static

# Return plain text, not HTML
contentType: text/plain

cache:
  enabled: true
  ttl: 86400  # 24 hours

# Default configuration
input:
  disallowAll: false
  disallowPaths:
    - /api/*
    - /admin/*
    - /_*
  crawlDelay: null
  sitemap: ${env.PUBLIC_URL}/sitemap.xml
